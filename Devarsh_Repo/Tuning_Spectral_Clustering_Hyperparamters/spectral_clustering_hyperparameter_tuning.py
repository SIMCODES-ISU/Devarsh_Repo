# -*- coding: utf-8 -*-
"""Spectral Clustering Hyparameter tuning

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1tHTWoahAEG7GeuX21dxsNUA-lvFJzCC2
"""

# !pip install xyz2graph
# !pip install "numpy<2.0"
# !pip install scipy
# !pip install joblib
# !pip install scikit-learn
import matplotlib.pyplot as plt
from xyz2graph import MolGraph
from networkx.algorithms.community import louvain_communities, modularity
from networkx import adjacency_matrix
from scipy.sparse import csr_matrix
from sklearn.cluster import KMeans
import networkx as nx
import numpy as np
import subprocess  # Run xTB
import os  # Run xTB
import csv  # log information
import re  # for extracting output
from joblib import Parallel, delayed  # Parallel runs for xTB
import math


# Additions in version 1.3 :
# Added parallelization of xTB runs
# Added out_ to all output files
# Added minutes for wall_time
#  Add more clusters in spectral clustering
#  Have a soft cap of fragment size (keep if at max 60ish)

# Pre-Processing Inputs

def MakeGraph(File_Path):
    mg = MolGraph()
    mg.read_xyz(File_Path)
    graph = mg.to_networkx()
    return graph


def MakeAdjecencyMatrix(File_Path):
    mg = MolGraph()
    mg.read_xyz(File_Path)
    graph = mg.to_networkx()
    adj_graph = adjacency_matrix(graph)
    return adj_graph  # Provides a Adj_Matrix to work with


# Steps for Spectral Clustering

def spectral_cut2(A_dense, number_of_clusters):
    n = A_dense.shape[0]  # Gets Number of Nodes
    degrees = A_dense.sum(axis=1)  # List of Degree of each node
    d = np.diag(degrees)  # Makes diagonal matrix from Degree
    L = d - A_dense  # Makes Laplacian Matrix
    w, v = np.linalg.eigh(L)  # finds eigenvalues and eigenvectors
    eigvecs = v[:, 1:number_of_clusters + 1]
    kmeans = KMeans(n_clusters=number_of_clusters, n_init=100)
    labels = kmeans.fit_predict(eigvecs)

    cluster_dict = {}
    index = 0
    labels = list(labels)
    for i in labels:
        if i not in cluster_dict:
            cluster_dict[i] = set()
        cluster_dict[i].add(index)
        index += 1
    fragments = list(cluster_dict.values())

    return fragments  # a list of sets of fragments


def visualisation(communities, Graph, Model="ERR"):
    node_to_comm = {}  # More storage but faster look up
    comm_id = 0
    for comm in communities:  # divives all nodes into communites
        for node in comm:
            node_to_comm[node] = comm_id
        comm_id += 1

    edges_to_cut = []
    edges_to_keep = []
    for u, v in Graph.edges():
        if node_to_comm[u] != node_to_comm[v]:  #If connects 2 clusters
            edges_to_cut.append((u, v))
        else:
            edges_to_keep.append((u, v))
    return edges_to_cut


def read_xyz(File_Path):  # auxiliary function
    with open(File_Path, 'r') as f:
        lines = f.readlines()
    atoms = []
    coords = []
    for line in lines[2:]:  # skip count and comment lines
        parts = line.strip().split()
        atoms.append(parts[0])
        coords.append(np.array([float(x) for x in parts[1:4]]))
    return atoms, coords


def get_fragment_atom_lists_with_caps(File_Path, fragment_list, cut_edges):
    atoms, coords = read_xyz(File_Path)

    fragments = [[] for _ in range(len(fragment_list))]

    for frag_idx in range(len(fragment_list)):
        fragment = fragment_list[frag_idx]
        for j in fragment:
            fragments[frag_idx].append((atoms[j], coords[j]))
            # Check cut edges involving atom j
            for edge in cut_edges:
                if j in edge:
                    other = edge[1] if edge[0] == j else edge[0]
                    bond_vector = np.array(coords[other]) - np.array(
                        coords[j])  # using vector math to place H(cap) at the right place
                    unit_vector = bond_vector / np.linalg.norm(bond_vector)
                    h_position = np.array(
                        coords[j]) + 1 * unit_vector  # Closer to actual distance, helps Molgraph make a bond
                    fragments[frag_idx].append(('H', h_position.tolist()))  # Add Distance Dictionary here

    return fragments


def write_xyz_fragments(fragments, File_Path, model="ERR"):
    base_name = os.path.splitext(File_Path)[0]
    file_list = [File_Path]

    for idx, frag in enumerate(fragments):
        file_name = f"out_{base_name}_Frag_{model}_{idx + 1}.xyz"
        with open(file_name, "w") as f:
            f.write(f"{len(frag)}\n")  # Line 1: number of atoms
            f.write(f"Fragment {idx + 1}\n")  # Line 2: comment
            for atom, coord in frag:
                x, y, z = coord
                f.write(f"{atom} {x:.3f} {y:.3f} {z:.3f}\n")
        file_list.append(file_name)

    return file_list


def Run_Spectral_Clustering(File_Path):
    Matrix_SC = MakeAdjecencyMatrix(File_Path)  # Recieves Adj_Matrix from xyz file
    Graph_viz_SC = MakeGraph(File_Path)  # Graph for visaulaisation run
    fragment_list_SC = adaptive_clustering(Matrix_SC, "SC")  # allows to impsoe a limit on avg. fragment length
    cut_edges_SC = visualisation(fragment_list_SC, Graph_viz_SC, "SC")
    #print(cut_edges_SC)
    fragments_final_SC = get_fragment_atom_lists_with_caps(File_Path, fragment_list_SC, cut_edges_SC)
    #print(fragments_final_SC)
    file_list_SC = write_xyz_fragments(fragments_final_SC, File_Path, "SC")
    return file_list_SC, cut_edges_SC

# Running the xTB Model and fetching outputs


def extract_energy_and_time(output):
    energy_match = re.search(r'TOTAL ENERGY\s+(-?\d+\.\d+)', output)
    time_match = re.search(r'wall-time:\s+\d+ d,\s+\d+ h,\s+(\d+)\s+min,\s+([\d.]+)\s+sec', output)

    energy = float(energy_match.group(1)) if energy_match else None
    time = (int(time_match.group(1)) * 60 + float(
        time_match.group(2))) if time_match else None  # Now handels outputs in mins and seconds

    return energy, time


cap_correction_per_2h = 0.982686139534  # Define Gloabally so function can use joblib.parallel


def run_fragment(frag):
    frag_output = subprocess.run(
        ["xtb", frag, "--gfn2"],
        capture_output=True,
        text=True).stdout

    energy, time = extract_energy_and_time(frag_output)
    return energy, time


def run_fragments_joblib(file_list, cut_edges, n_jobs=8):  # depends on cpus per core requested
    cap_energy = cap_correction_per_2h * len(cut_edges)

    results = Parallel(n_jobs=n_jobs)(
        delayed(run_fragment)(frag) for frag in
        file_list)  # initiates parallel runs for run_fragment (speeding up the process)

    total_energy = cap_energy
    max_time = 0
    for energy, time in results:
        if energy is not None:
            total_energy += energy
            if time is not None:
                max_time = max(max_time, time)
    return total_energy, max_time


def run_xtb_and_log(file_list, cut_edges_LM, cut_edges_SC, csv_path="xtb_log.csv"):  # name for CSV file

    base_file = file_list[0]
    fragment_files = file_list[1:]

    lm_fragments = [f for f in fragment_files if "_Frag_LM_" in f]  #Seperate out SC and LM
    sc_fragments = [f for f in fragment_files if "_Frag_SC_" in f]

    base_output = subprocess.run(["xtb", base_file, "--gfn2"], capture_output=True, text=True).stdout
    base_energy, base_time = extract_energy_and_time(base_output)  # Gets and stores ground output
    lm_energy, lm_max_time = run_fragments_joblib(lm_fragments, cut_edges_LM)  # runs for each fragment
    sc_energy, sc_max_time = run_fragments_joblib(sc_fragments, cut_edges_SC)

    row = [
        os.path.basename(base_file),
        base_energy,
        base_time,
        lm_energy,
        lm_max_time,
        sc_energy,
        sc_max_time]
    write_header = not os.path.exists(csv_path)
    with open(csv_path, "a", newline="") as f:
        writer = csv.writer(f)
        if write_header:
            writer.writerow([
                "base file name",
                "ground energy",
                "ground time",
                "cumulated fragment energy (LM)",
                "max fragment time (LM)",
                "cumulated fragment energy (SC)",
                "max fragment time (SC)"
            ])
        writer.writerow(row)


def get_num_clusters_list(graph_size, min_clusters=2, min_frag_size=13, max_clusters_buffer=7):
    if graph_size < min_frag_size:
        # If graph is very small, limit cluster numbers strictly
        return list(range(2, min(5, graph_size)))

    # Maximum clusters should never exceed number of nodes - 1
    max_clusters = min(graph_size - 1, graph_size // min_frag_size + max_clusters_buffer)

    # Construct the cluster list with a widespread
    num_clusters_list = list(range(min_clusters, max_clusters + 1))

    return num_clusters_list


def auto_tune_clustering(File_Path, max_runs=100, target_diff=0.01):

    run_count_SC = 0

    G = MakeGraph(File_Path)
    graph_size = len(G.nodes)
    num_clusters_list = get_num_clusters_list(graph_size)

    # Get base energy
    base_output = subprocess.run(["xtb", File_Path, "--gfn2"], capture_output=True, text=True).stdout
    base_energy, base_time = extract_energy_and_time(base_output)

    min_diff_for_thershold_SC = float('inf')
    Found_Candidate_SC = False
    temp_params_SC = {}
    best_params_SC = {}

    ### --- Spectral Clustering --- ###

    for num_clusters in num_clusters_list:
        run_count_SC += 1

        A = MakeAdjecencyMatrix(File_Path)
        G_viz = MakeGraph(File_Path)
        # sc_frags = adaptive_clustering(A, "SC", threshold=threshold, max_attempts=10, resolution=float('inf'),
        #                                num_clusters=num_clusters)   # resolution is ignored for SC
        sc_frags = spectral_cut2(A.toarray(), num_clusters)
        cut_edges_SC = visualisation(sc_frags, G_viz, "SC")
        fragments_SC = get_fragment_atom_lists_with_caps(File_Path, sc_frags, cut_edges_SC)
        file_list_SC = write_xyz_fragments(fragments_SC, File_Path, "SC")

        sc_energy, sc_time = run_fragments_joblib(file_list_SC[1:], cut_edges_SC)

        diff = abs(base_energy - sc_energy)
        if diff < target_diff:
            best_params_SC = {
                "File": File_Path,
                "Model": "SC",
                "Number of Clusters": num_clusters,
                "energy": sc_energy,
                "energy_diff": diff}
            Found_Candidate_SC = True
            break

        if diff < min_diff_for_thershold_SC:
            min_diff_for_thershold_SC = diff
            temp_params_SC = {
                "File": File_Path,
                "Model": "SC",
                "Number of Clusters": num_clusters,
                "energy": sc_energy,
                "energy_diff": diff
            }
        if run_count_SC == max_runs:
            print("Reached max runs limit.")
            break

        # Clean up .xyz files
        for file in file_list_SC[1:]:
            if os.path.exists(file):
                os.remove(file)

    if not Found_Candidate_SC:
        print("Didn't Converge, ", temp_params_SC)
        file_exists = os.path.isfile("SC_Not_Converged.csv")
        with open("SC_Not_Converged.csv", mode='a', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=temp_params_SC.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(temp_params_SC)

    if Found_Candidate_SC:
        print("SC Converged", best_params_SC)
        file_exists = os.path.isfile("SC_Converged.csv")
        with open("SC_Converged.csv", mode='a', newline='') as file:
            writer = csv.DictWriter(file, fieldnames=best_params_SC.keys())
            if not file_exists:
                writer.writeheader()
            writer.writerow(best_params_SC)


File_Path = input("File Path :")
auto_tune_clustering(File_Path)
